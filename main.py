import time
import random
import pandas as pd
from urllib.parse import urlparse, parse_qs, unquote
from playwright.sync_api import sync_playwright

# ==========================================
# [ì„¤ì • 1] ëª¨ë‹ˆí„°ë§ íƒ€ê²Ÿ
# ==========================================
MONITORING_TARGETS = [
    {"keyword": "í† ìµ", "target_url": "https://www.hackers.co.kr"},
    {"keyword": "ê³µì¸ì¤‘ê°œì‚¬ì‹œí—˜", "target_url": "https://land.hackers.com"},
    {"keyword": "ê³µë¬´ì›ì‹œí—˜", "target_url": "https://gosi.hackers.com"},
    {"keyword": "ê²½ì°°ê³µë¬´ì›ì‹œí—˜", "target_url": "https://police.hackers.com"}
]

TARGET_COLLECT_COUNT = 50  # ìˆ˜ì§‘í•  ê¸°ì‚¬ ë§í¬ ê°œìˆ˜
TARGET_AD_FOUND_LIMIT = 10 # [ì‹ ê·œ] ê´‘ê³  ë°œê²¬ 10ê°œ ì±„ìš°ë©´ 1ì°¨ ê²€ì‚¬ ì¤‘ë‹¨
MAX_CHECK_LIMIT = 50       # ìµœëŒ€ ê²€ì‚¬ í•œë„

# ==========================================
# [ì„¤ì • 2] ê´‘ê³  ë§¤ì²´ ì½”ë“œ
# ==========================================
NETWORK_MAPPING = {
    "googleads": "G", "doubleclick": "G", "googlesyndication": "G",
    "criteo": "C",
    "widerplanet": "M", "mobon": "M",
    "daum": "K", "kakao": "K",
    "tg360": "T", "targetinggates": "T",
    "acetrader": "A", "acecounter": "A"
}
DISPLAY_NETWORKS = ["G", "C", "K", "M", "T", "A"]

# ==========================================
# [ì„¤ì • 3] ê²½ìŸì‚¬ ëª©ë¡
# ==========================================
COMPETITORS = {
    "í•´ì»¤ìŠ¤": ["hackers", "champstudy"],
    "ì—ë“€ìœŒ": ["eduwill"],
    "YBM": ["ybm"],
    "íŒŒê³ ë‹¤": ["pagoda"],
    "ì˜ë‹¨ê¸°": ["dangi"],
    "ê³µë‹¨ê¸°": ["gong.dangi"],
    "ë°•ë¬¸ê°": ["pmg", "bakmun"],
    "ë©”ê°€": ["megaland", "mega.co.kr"],
    "ì•¼ë‚˜ë‘": ["yanadoo"],
    "ì‹œì›ìŠ¤ì¿¨": ["siwon"]
}
DISPLAY_COMPANIES = list(COMPETITORS.keys())

def get_clean_url(naver_redirect_url):
    if "search.naver.com/p/crd" in naver_redirect_url:
        try:
            parsed = urlparse(naver_redirect_url)
            query = parse_qs(parsed.query)
            if 'u' in query: return unquote(query['u'][0])
        except: pass
    return naver_redirect_url

def remove_mobon_icover(page):
    """ëª¨ë¹„ì˜¨ ì•„ì´ì»¤ë²„(ì „ë©´ê´‘ê³ ) ì‚­ì œ"""
    try:
        close_selectors = [
            "#mobon_icover .btn_close", 
            "#mobon_icover button",
            "div[id*='mobon'] .close",
            ".mobon_cover .btn_close"
        ]
        
        for selector in close_selectors:
            if page.locator(selector).is_visible():
                # print("  ğŸ›¡ï¸ ëª¨ë¹„ì˜¨ ë‹«ê¸° í´ë¦­")
                page.locator(selector).click(force=True)
                time.sleep(0.5)
                return

        page.evaluate("""() => {
            const elements = document.querySelectorAll("div, iframe");
            elements.forEach(el => {
                if (el.id.includes('mobon') || el.className.includes('mobon')) {
                    if (el.style.display !== 'none') {
                        el.remove();
                    }
                }
            });
        }""")
    except:
        pass

def analyze_ads_count(page):
    """ê´‘ê³  ê°œìˆ˜ ë° ë°œê²¬ëœ ê²½ìŸì‚¬ ë¶„ì„"""
    counts = {comp: {net: 0 for net in DISPLAY_NETWORKS} for comp in COMPETITORS}
    
    remove_mobon_icover(page)
    
    for frame in page.frames:
        try:
            frame_url = frame.url.lower()
            try: frame_content = frame.content().lower()
            except: frame_content = ""

            detected_net_code = None
            detected_company = None

            for keyword, code in NETWORK_MAPPING.items():
                if keyword in frame_url:
                    detected_net_code = code
                    break
            
            if detected_net_code and detected_net_code in DISPLAY_NETWORKS:
                for comp_name, keywords in COMPETITORS.items():
                    if any(k in frame_url for k in keywords) or any(k in frame_content for k in keywords):
                        detected_company = comp_name
                        break
                
                if detected_company:
                    counts[detected_company][detected_net_code] += 1
        except: continue
        
    return counts

def run_monitoring():
    start_time = time.time()
    total_data = {}

    with sync_playwright() as p:
        print("ğŸš€ [ìµœì¢…] ëª¨ë‹ˆí„°ë§ ì‹œì‘ (í™”ë©´í‘œì‹œ ON / 10ê°œ ë°œê²¬ ì‹œ ì¡°ê¸° ì¢…ë£Œ)")
        
        # í™”ë©´ì— ë³´ì´ë„ë¡ ì„¤ì • (0,0)
        browser = p.chromium.launch(
            channel="msedge", headless=False,
            args=["--window-position=0,0", "--disable-blink-features=AutomationControlled"]
        )
        context = browser.new_context(viewport={'width': 1920, 'height': 1080})
        context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        page = context.new_page()

        for item in MONITORING_TARGETS:
            keyword = item["keyword"]
            target_url = item["target_url"]
            context.clear_cookies()
            
            print(f"\n==================================================")
            print(f"ğŸ” í‚¤ì›Œë“œ: '{keyword}' ì‘ì—… ì‹œì‘")
            print(f"==================================================")
            
            # -------------------------------------------------
            # 1. ë§í¬ ìˆ˜ì§‘ (ìŠ¤í¬ë¡¤ ì ìš©)
            # -------------------------------------------------
            search_url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Aall&is_sug_officeid=0"
            page.goto(search_url, wait_until="domcontentloaded")
            
            collected_articles = []
            page_num = 1
            
            while len(collected_articles) < TARGET_COLLECT_COUNT:
                try:
                    # ìŠ¤í¬ë¡¤ ìµœí•˜ë‹¨ ì´ë™
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(1.5)
                    page.keyboard.press("End")
                    time.sleep(1)

                    try: page.wait_for_selector('[data-heatmap-target=".tit"]', timeout=3000)
                    except: page.wait_for_selector(".news_tit", timeout=3000)

                    new_links = page.evaluate("""() => {
                        let nodes = document.querySelectorAll('[data-heatmap-target=".tit"]');
                        if (nodes.length === 0) nodes = document.querySelectorAll('.news_tit');
                        return Array.from(nodes).map(a => ({text: a.innerText, url: a.href}));
                    }""")
                    
                    prev_len = len(collected_articles)
                    for link in new_links:
                        if not any(saved['url'] == link['url'] for saved in collected_articles):
                            collected_articles.append(link)
                    
                    print(f"  > {page_num}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... (ëˆ„ì  {len(collected_articles)}/{TARGET_COLLECT_COUNT}ê°œ)")
                    
                    if len(collected_articles) >= TARGET_COLLECT_COUNT:
                        break
                    
                    # ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if len(collected_articles) == prev_len and page_num > 1:
                         # ë§ˆì§€ë§‰ í™•ì¸ ì‚¬ì‚´
                        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                        time.sleep(2)
                        next_btn_check = page.locator(".btn_next")
                        if next_btn_check.count() == 0 or next_btn_check.get_attribute("aria-disabled") == "true":
                            print("  > ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            break

                    next_btn = page.locator(".btn_next")
                    if next_btn.count() > 0 and next_btn.get_attribute("aria-disabled") != "true":
                        remove_mobon_icover(page) 
                        page.evaluate("document.querySelector('.btn_next').click()")
                        page_num += 1
                        time.sleep(2.5)
                    else:
                        break
                except Exception as e:
                    print(f"  âš ï¸ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
                    break

            collected_articles = collected_articles[:TARGET_COLLECT_COUNT]
            print(f"  > ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ. 1ì°¨ ë¶„ì„ ì‹œì‘ (ëª©í‘œ: ê´‘ê³  ë°œê²¬ {TARGET_AD_FOUND_LIMIT}ê±´)")

            # -------------------------------------------------
            # 2. [1ì°¨] ë°©ë¬¸ ì „ ê²€ì‚¬ (10ê°œ ì°¾ìœ¼ë©´ STOP)
            # -------------------------------------------------
            target_articles = [] # ì‹¤ì œ ê´‘ê³ ê°€ ë°œê²¬ëœ URL ë¦¬ìŠ¤íŠ¸
            found_ad_count = 0   # ì°¾ì€ ê´‘ê³  ê°œìˆ˜ ì¹´ìš´í„°

            for i, article in enumerate(collected_articles):
                # 10ê°œ ì°¾ì•˜ìœ¼ë©´ ë£¨í”„ íƒˆì¶œ
                if found_ad_count >= TARGET_AD_FOUND_LIMIT:
                    print(f"  ğŸ›‘ ëª©í‘œ ê´‘ê³  {TARGET_AD_FOUND_LIMIT}ê°œë¥¼ ëª¨ë‘ ì°¾ì•˜ìŠµë‹ˆë‹¤. 1ì°¨ ê²€ì‚¬ ì¢…ë£Œ.")
                    break
                
                real_url = get_clean_url(article['url'])
                if not real_url.startswith("http"): continue
                
                if real_url not in total_data:
                    total_data[real_url] = {
                        'info': {'í‚¤ì›Œë“œ': keyword, 'ê¸°ì‚¬ì œëª©': article['text']},
                        'before': {}, 'after': {}
                    }

                print(f"  > [ë°©ë¬¸ì „] {i+1}ë²ˆì§¸ ê¸°ì‚¬ í™•ì¸ ì¤‘...", end="")
                try:
                    page.goto(real_url, timeout=15000, wait_until="domcontentloaded")
                    remove_mobon_icover(page)
                    
                    # ê´‘ê³  ë¡œë”© ìœ ë„ (ìŠ¤í¬ë¡¤)
                    page.keyboard.press("End")
                    time.sleep(2.5)
                    for _ in range(2): 
                        page.mouse.wheel(0, -1000)
                        time.sleep(0.2)
                        remove_mobon_icover(page) 
                    
                    counts = analyze_ads_count(page)
                    total_data[real_url]['before'] = counts
                    
                    # ë°œê²¬ëœ íšŒì‚¬ ì°¾ê¸°
                    found_companies = []
                    for comp, nets in counts.items():
                        if sum(nets.values()) > 0:
                            found_companies.append(comp)
                    
                    if found_companies:
                        print(f" âœ… ê´‘ê³ ë°œê²¬ ({', '.join(found_companies)})")
                        target_articles.append(real_url)
                        found_ad_count += 1
                    else:
                        print(f" (íƒ€ì‚¬ ì—†ìŒ)")
                        
                except: print(" âš ï¸ ì—ëŸ¬/íŒ¨ìŠ¤")
                time.sleep(0.5)

            if not target_articles:
                print("  âš ï¸ ë°œê²¬ëœ íƒ€ì‚¬ ê´‘ê³ ê°€ ì—†ì–´ 2ì°¨ ê²€ì‚¬ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
                continue

            # -------------------------------------------------
            # 3. íƒ€ì‚¬ ì‚¬ì´íŠ¸ ë°©ë¬¸ (ì¿ í‚¤ ìƒì„±)
            # -------------------------------------------------
            print(f"  > [ì¿ í‚¤ ì‘ì—…] ê²½ìŸì‚¬ íƒ€ê²Ÿ ì‚¬ì´íŠ¸ ë°©ë¬¸: {target_url}")
            try:
                page.goto(target_url)
                time.sleep(4)
                page.mouse.wheel(0, 1000)
                time.sleep(1)
            except: pass

            # -------------------------------------------------
            # 4. [2ì°¨] ë°©ë¬¸ í›„ ê²€ì‚¬ (ì°¾ì•˜ë˜ 10ê°œë§Œ ë‹¤ì‹œ í™•ì¸)
            # -------------------------------------------------
            print(f"  > [ë°©ë¬¸í›„] ë°œê²¬í–ˆë˜ {len(target_articles)}ê°œ ê¸°ì‚¬ ì¬í™•ì¸ ì‹œì‘...")
            for url in target_articles:
                print(f"  > ì¬ì§„ì…: {url[:40]}...", end="")
                try:
                    page.goto(url, timeout=15000, wait_until="domcontentloaded")
                    remove_mobon_icover(page)
                    
                    page.keyboard.press("End")
                    time.sleep(2.5)
                    for _ in range(2): 
                        page.mouse.wheel(0, -1000)
                        remove_mobon_icover(page)
                    
                    counts = analyze_ads_count(page)
                    total_data[url]['after'] = counts
                    
                    # ì¬ì§„ì… ì‹œ ë°œê²¬ëœ íšŒì‚¬
                    found_companies = [comp for comp, nets in counts.items() if sum(nets.values()) > 0]
                    if found_companies:
                        print(f" âœ… ({', '.join(found_companies)})")
                    else:
                        print(" (ì‚¬ë¼ì§/íƒ€ì‚¬ì—†ìŒ)")
                        
                except: print(" âš ï¸ ì‹¤íŒ¨")
                time.sleep(0.5)

        browser.close()

    # ì—‘ì…€ ì €ì¥
    print("\nğŸ“Š ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘...")
    columns = [('ê¸°ë³¸ì •ë³´', 'ê¸°ë³¸ì •ë³´', 'í‚¤ì›Œë“œ'), ('ê¸°ë³¸ì •ë³´', 'ê¸°ë³¸ì •ë³´', 'ê¸°ì‚¬ì œëª©'), ('ê¸°ë³¸ì •ë³´', 'ê¸°ë³¸ì •ë³´', 'URL')]
    
    for comp in DISPLAY_COMPANIES:
        for phase in ['ì¿ í‚¤ê°’ ì‚­ì œ', 'ë°©ë¬¸ í›„']:
            for net in DISPLAY_NETWORKS:
                columns.append((comp, phase, net))
    
    multi_columns = pd.MultiIndex.from_tuples(columns, names=['íšŒì‚¬', 'ì‹œê¸°', 'ë§¤ì²´'])
    
    rows = []
    # ë°ì´í„°ê°€ ìˆëŠ” ê²ƒë§Œ ì €ì¥ (Pre-visitì—ì„œ 10ê°œë§Œ ëŒë ¸ìœ¼ë©´ 10ê°œë§Œ ì €ì¥ë¨)
    for url, data in total_data.items():
        # ìˆ˜ì§‘ì€ í–ˆìœ¼ë‚˜ ë°©ë¬¸í•˜ì§€ ì•Šì•„ ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì œì™¸
        if not data['before'] and not data['after']:
            continue
            
        row_data = [data['info']['í‚¤ì›Œë“œ'], data['info']['ê¸°ì‚¬ì œëª©'], url]
        
        for comp in DISPLAY_COMPANIES:
            # ë°©ë¬¸ ì „
            before = data['before'].get(comp, {n:0 for n in DISPLAY_NETWORKS})
            for net in DISPLAY_NETWORKS:
                cnt = before.get(net, 0)
                row_data.append(cnt if cnt > 0 else "")
            
            # ë°©ë¬¸ í›„
            after = data['after'].get(comp, {n:0 for n in DISPLAY_NETWORKS})
            for net in DISPLAY_NETWORKS:
                cnt = after.get(net, 0)
                row_data.append(cnt if cnt > 0 else "")
                
        rows.append(row_data)

    df = pd.DataFrame(rows, columns=multi_columns)
    file_name = f"ë°°ë„ˆëª¨ë‹ˆí„°ë§_ìµœì¢…ì™„ë£Œ_{time.strftime('%H%M%S')}.xlsx"
    df.to_excel(file_name)
    
    end_time = time.time()
    elapsed = end_time - start_time
    print(f"\nğŸ‰ [ì™„ë£Œ] íŒŒì¼ ì €ì¥ë¨: {file_name}")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {int(elapsed//60)}ë¶„ {int(elapsed%60)}ì´ˆ")

if __name__ == "__main__":
    run_monitoring()
